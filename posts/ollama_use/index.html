<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ollama set up and my use cases | Timur Coding Stuff</title>
<meta name=keywords content="macbook pro,ollama,llm"><meta name=description content="After I successfully installed Ventura to my old mac (see previous post) I went straight to installing Ollama and trying models.
I have used following youtube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4 and I can say it is really good. And Ollama documentation is great as well: https://github.com/ollama/ollama/blob/main/README.md.
So far I tried running llama 3.2 7b and codegemma 7b. After getting used to chat gpt this seems like crazy slow. But, given that I have specific usecases in mind that might still fit me well."><meta name=author content><link rel=canonical href=https://timurvafin.blog/posts/ollama_use/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://timurvafin.blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://timurvafin.blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://timurvafin.blog/favicon-32x32.png><link rel=apple-touch-icon href=https://timurvafin.blog/apple-touch-icon.png><link rel=mask-icon href=https://timurvafin.blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://timurvafin.blog/posts/ollama_use/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://timurvafin.blog/posts/ollama_use/"><meta property="og:site_name" content="Timur Coding Stuff"><meta property="og:title" content="Ollama set up and my use cases"><meta property="og:description" content="After I successfully installed Ventura to my old mac (see previous post) I went straight to installing Ollama and trying models. I have used following youtube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4 and I can say it is really good. And Ollama documentation is great as well: https://github.com/ollama/ollama/blob/main/README.md.
So far I tried running llama 3.2 7b and codegemma 7b. After getting used to chat gpt this seems like crazy slow. But, given that I have specific usecases in mind that might still fit me well."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-15T18:42:25+11:00"><meta property="article:modified_time" content="2025-02-15T18:42:25+11:00"><meta property="article:tag" content="Macbook Pro"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="Llm"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ollama set up and my use cases"><meta name=twitter:description content="After I successfully installed Ventura to my old mac (see previous post) I went straight to installing Ollama and trying models.
I have used following youtube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4 and I can say it is really good. And Ollama documentation is great as well: https://github.com/ollama/ollama/blob/main/README.md.
So far I tried running llama 3.2 7b and codegemma 7b. After getting used to chat gpt this seems like crazy slow. But, given that I have specific usecases in mind that might still fit me well."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://timurvafin.blog/posts/"},{"@type":"ListItem","position":2,"name":"Ollama set up and my use cases","item":"https://timurvafin.blog/posts/ollama_use/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ollama set up and my use cases","name":"Ollama set up and my use cases","description":"After I successfully installed Ventura to my old mac (see previous post) I went straight to installing Ollama and trying models. I have used following youtube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4 and I can say it is really good. And Ollama documentation is great as well: https://github.com/ollama/ollama/blob/main/README.md.\nSo far I tried running llama 3.2 7b and codegemma 7b. After getting used to chat gpt this seems like crazy slow. But, given that I have specific usecases in mind that might still fit me well.\n","keywords":["macbook pro","ollama","llm"],"articleBody":"After I successfully installed Ventura to my old mac (see previous post) I went straight to installing Ollama and trying models. I have used following youtube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4 and I can say it is really good. And Ollama documentation is great as well: https://github.com/ollama/ollama/blob/main/README.md.\nSo far I tried running llama 3.2 7b and codegemma 7b. After getting used to chat gpt this seems like crazy slow. But, given that I have specific usecases in mind that might still fit me well.\nMy use cases 1. Bookmarks A while ago I created a soft that served me as a bookmark manager. It had two parts - telegram bot and a web interface. The way how it worked was you send a web url and python would scrape the website for the title and assign tags based on some rules. For example if domain name is youtube it assigned “video”, if title had words like “python” it assigned “programming” and “python”. It used to run on Heroku until it had free tier. And telegram bot used to run on my raspberry pi at home. The database however is cloud monge, so it is the only thing that is alive now. What I would like to do next is create a new version of the bookmarks, and the way how I plan to use LLM is to summarise the content of the page and auto-assign tags. We’ll see how it would work. I’ll dedicate a separate set of posts under the tag “bookmarx”\n2. Document retriever I have quiet a lot of different documents and sometimes I simply forgot were are they and how to find them. Moreover, I sometimes may need only part of the info from that document. For example, I need to run descaling for my coffee machine time to time. And I do not store that info in my head as I know that there is a manual or this info can be googled. But I find google becomes worse nowadays, and sometimes it takes like 10 minutes to find what you want. Or, another example will be my personal documents like visas or emails or flight info. That stuff is buried in the gmail and, unfortunately, my gmail has a lot in it so search finds everything - both the stuff I need and the stuff I don’t\n3. Expenses and budget planner I’ve created my own budget planner app and it was quiet good in terms of ability to create different scenarious and so on. The main issue that I found is that without actual expenses it is tough to make any judjements about how you are doing. The reason why I haven’t used any solution on the market is lack of trust. Plus I do use multiple banks for different reasons, so having all info in the one place would be neat. So what I tried to do is created a custom CV model that would read the PDFs that my bank exports and put it as a list of transactions. Unfortunately, even within single bank the output of the PDF is not consistent and some adjustments required. I was thinknig on reying a multi-modal llm to try to combat this issue and to have at least all thransactions in a table format for a start.\n","wordCount":"550","inLanguage":"en","datePublished":"2025-02-15T18:42:25+11:00","dateModified":"2025-02-15T18:42:25+11:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://timurvafin.blog/posts/ollama_use/"},"publisher":{"@type":"Organization","name":"Timur Coding Stuff","logo":{"@type":"ImageObject","url":"https://timurvafin.blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://timurvafin.blog/ accesskey=h title="Timur Coding Stuff (Alt + H)"><img src=https://timurvafin.blog/apple-touch-icon.png alt aria-label=logo height=35>Timur Coding Stuff</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://timurvafin.blog/posts/ title=posts><span>posts</span></a></li><li><a href=https://timurvafin.blog/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://timurvafin.blog/>Home</a>&nbsp;»&nbsp;<a href=https://timurvafin.blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Ollama set up and my use cases</h1><div class=post-meta><span title='2025-02-15 18:42:25 +1100 +1100'>February 15, 2025</span>&nbsp;·&nbsp;3 min</div></header><div class=post-content><p>After I successfully installed Ventura to my old mac (see previous post) I went straight to installing Ollama and trying models.
I have used following youtube tutorial: <a href="https://www.youtube.com/watch?v=GWB9ApTPTv4">https://www.youtube.com/watch?v=GWB9ApTPTv4</a> and I can say it is really good. And Ollama documentation is great as well: <a href=https://github.com/ollama/ollama/blob/main/README.md>https://github.com/ollama/ollama/blob/main/README.md</a>.</p><p>So far I tried running llama 3.2 7b and codegemma 7b. After getting used to chat gpt this seems like crazy slow. But, given that I have specific usecases in mind that might still fit me well.</p><h2 id=my-use-cases>My use cases<a hidden class=anchor aria-hidden=true href=#my-use-cases>#</a></h2><h3 id=1-bookmarks>1. Bookmarks<a hidden class=anchor aria-hidden=true href=#1-bookmarks>#</a></h3><p>A while ago I created a soft that served me as a bookmark manager.
It had two parts - telegram bot and a web interface. The way how it worked was you send a web url and python would scrape the website for the title and assign tags based on some rules.
For example if domain name is youtube it assigned &ldquo;video&rdquo;, if title had words like &ldquo;python&rdquo; it assigned &ldquo;programming&rdquo; and &ldquo;python&rdquo;. It used to run on Heroku until it had free tier. And telegram bot used to run on my raspberry pi at home. The database however is cloud monge, so it is the only thing that is alive now. What I would like to do next is create a new version of the bookmarks, and the way how I plan to use LLM is to summarise the content of the page and auto-assign tags. We&rsquo;ll see how it would work. I&rsquo;ll dedicate a separate set of posts under the tag &ldquo;bookmarx&rdquo;</p><h3 id=2-document-retriever>2. Document retriever<a hidden class=anchor aria-hidden=true href=#2-document-retriever>#</a></h3><p>I have quiet a lot of different documents and sometimes I simply forgot were are they and how to find them. Moreover, I sometimes may need only part of the info from that document. For example, I need to run descaling for my coffee machine time to time. And I do not store that info in my head as I know that there is a manual or this info can be googled. But I find google becomes worse nowadays, and sometimes it takes like 10 minutes to find what you want. Or, another example will be my personal documents like visas or emails or flight info. That stuff is buried in the gmail and, unfortunately, my gmail has a lot in it so search finds everything - both the stuff I need and the stuff I don&rsquo;t</p><h3 id=3-expenses-and-budget-planner>3. Expenses and budget planner<a hidden class=anchor aria-hidden=true href=#3-expenses-and-budget-planner>#</a></h3><p>I&rsquo;ve created my own budget planner app and it was quiet good in terms of ability to create different scenarious and so on. The main issue that I found is that without actual expenses it is tough to make any judjements about how you are doing. The reason why I haven&rsquo;t used any solution on the market is lack of trust. Plus I do use multiple banks for different reasons, so having all info in the one place would be neat. So what I tried to do is created a custom CV model that would read the PDFs that my bank exports and put it as a list of transactions. Unfortunately, even within single bank the output of the PDF is not consistent and some adjustments required. I was thinknig on reying a multi-modal llm to try to combat this issue and to have at least all thransactions in a table format for a start.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://timurvafin.blog/tags/macbook-pro/>Macbook Pro</a></li><li><a href=https://timurvafin.blog/tags/ollama/>Ollama</a></li><li><a href=https://timurvafin.blog/tags/llm/>Llm</a></li></ul><nav class=paginav><a class=next href=https://timurvafin.blog/posts/upgadingmac/><span class=title>Next »</span><br><span>Upgrading OS on my MacbookPro Mid-2012</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://timurvafin.blog/>Timur Coding Stuff</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>