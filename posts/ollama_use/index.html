<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ollama set up and my use cases | Timur Coding Stuff</title>
<meta name=keywords content="macbook pro,ollama,llm"><meta name=description content="After successfully installing Ventura on my old Mac (see previous post), I went straight to installing Llama and trying out models. I used the following YouTube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4, which I found to be really good. The Llama documentation is also great: https://github.com/ollama/ollama/blob/main/README.md.
So far, I have tried running Llama 3.2 7B and CodeGemma 7B. After getting used to Chat GPT, this seems like a significant slowdown. However, given that I have specific use cases in mind, it might still be suitable for me."><meta name=author content><link rel=canonical href=https://timurvafin.blog/posts/ollama_use/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://timurvafin.blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://timurvafin.blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://timurvafin.blog/favicon-32x32.png><link rel=apple-touch-icon href=https://timurvafin.blog/apple-touch-icon.png><link rel=mask-icon href=https://timurvafin.blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://timurvafin.blog/posts/ollama_use/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://timurvafin.blog/posts/ollama_use/"><meta property="og:site_name" content="Timur Coding Stuff"><meta property="og:title" content="Ollama set up and my use cases"><meta property="og:description" content="After successfully installing Ventura on my old Mac (see previous post), I went straight to installing Llama and trying out models. I used the following YouTube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4, which I found to be really good. The Llama documentation is also great: https://github.com/ollama/ollama/blob/main/README.md.
So far, I have tried running Llama 3.2 7B and CodeGemma 7B. After getting used to Chat GPT, this seems like a significant slowdown. However, given that I have specific use cases in mind, it might still be suitable for me."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-15T18:42:25+11:00"><meta property="article:modified_time" content="2025-02-15T18:42:25+11:00"><meta property="article:tag" content="Macbook Pro"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="Llm"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ollama set up and my use cases"><meta name=twitter:description content="After successfully installing Ventura on my old Mac (see previous post), I went straight to installing Llama and trying out models. I used the following YouTube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4, which I found to be really good. The Llama documentation is also great: https://github.com/ollama/ollama/blob/main/README.md.
So far, I have tried running Llama 3.2 7B and CodeGemma 7B. After getting used to Chat GPT, this seems like a significant slowdown. However, given that I have specific use cases in mind, it might still be suitable for me."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://timurvafin.blog/posts/"},{"@type":"ListItem","position":2,"name":"Ollama set up and my use cases","item":"https://timurvafin.blog/posts/ollama_use/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ollama set up and my use cases","name":"Ollama set up and my use cases","description":"After successfully installing Ventura on my old Mac (see previous post), I went straight to installing Llama and trying out models. I used the following YouTube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4, which I found to be really good. The Llama documentation is also great: https://github.com/ollama/ollama/blob/main/README.md.\nSo far, I have tried running Llama 3.2 7B and CodeGemma 7B. After getting used to Chat GPT, this seems like a significant slowdown. However, given that I have specific use cases in mind, it might still be suitable for me.\n","keywords":["macbook pro","ollama","llm"],"articleBody":"After successfully installing Ventura on my old Mac (see previous post), I went straight to installing Llama and trying out models. I used the following YouTube tutorial: https://www.youtube.com/watch?v=GWB9ApTPTv4, which I found to be really good. The Llama documentation is also great: https://github.com/ollama/ollama/blob/main/README.md.\nSo far, I have tried running Llama 3.2 7B and CodeGemma 7B. After getting used to Chat GPT, this seems like a significant slowdown. However, given that I have specific use cases in mind, it might still be suitable for me.\nMy use cases 1. Bookmarks A while ago, I created software that served as a bookmark manager. It had two parts - a Telegram bot and a web interface. The way it worked was that you would send a web URL, and Python would scrape the website for the title and assign tags based on some rules. For example, if the domain name was YouTube, it assigned “video”, or if the title contained words like “Python”, it assigned “programming” and “Python”. It used to run on Heroku until they discontinued their free tier. The Telegram bot used to run on my Raspberry Pi at home. However, the database is still hosted on Cloud MongoDB, so that’s the only part that remains active now. What I would like to do next is create a new version of the bookmarks manager, and I plan to use LLM to summarize the content of each page and auto-assign tags. I’ll dedicate a separate series of posts under the tag “bookmarx” to this project.\n2. Document retriever I have a large number of different documents, and sometimes I simply forget where they are or how to find them. Moreover, I often only need a specific part of the information from that document. For example, I need to descale my coffee machine periodically, but I don’t store that information in my head because I know it’s available in the manual or can be found online. However, I’ve noticed that Google has become less effective lately, and sometimes it takes around 10 minutes to find what I’m looking for. Another example is my personal documents, such as visas, emails, or flight information, which are buried in my Gmail inbox. Unfortunately, my Gmail account contains a lot of unnecessary information, so the search results often include both relevant and irrelevant data.\n3. Expenses and budget planner I’ve created my own budget planner app, which was quite good at allowing me to create different scenarios. However, I found that without actual expense data, it’s difficult to make accurate judgments about my financial situation. The reason I haven’t used any existing solutions on the market is that I lack trust in them. Additionally, I use multiple banks for different purposes, so having all my information in one place would be convenient. To address this issue, I tried creating a custom CV model that could read the PDFs exported by my bank and convert them into a list of transactions. Unfortunately, even within a single bank, the output of the PDF is not consistent, requiring adjustments to be made. I was thinking of trying a multi-modal LLM to combat this issue and at least get all my transactions in a table format for starters.\n","wordCount":"536","inLanguage":"en","datePublished":"2025-02-15T18:42:25+11:00","dateModified":"2025-02-15T18:42:25+11:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://timurvafin.blog/posts/ollama_use/"},"publisher":{"@type":"Organization","name":"Timur Coding Stuff","logo":{"@type":"ImageObject","url":"https://timurvafin.blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://timurvafin.blog/ accesskey=h title="Timur Coding Stuff (Alt + H)"><img src=https://timurvafin.blog/apple-touch-icon.png alt aria-label=logo height=35>Timur Coding Stuff</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://timurvafin.blog/posts/ title=posts><span>posts</span></a></li><li><a href=https://timurvafin.blog/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://timurvafin.blog/>Home</a>&nbsp;»&nbsp;<a href=https://timurvafin.blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Ollama set up and my use cases</h1><div class=post-meta><span title='2025-02-15 18:42:25 +1100 +1100'>February 15, 2025</span>&nbsp;·&nbsp;3 min</div></header><div class=post-content><p>After successfully installing Ventura on my old Mac (see previous post), I went straight to installing Llama and trying out models. I used the following YouTube tutorial: <a href="https://www.youtube.com/watch?v=GWB9ApTPTv4">https://www.youtube.com/watch?v=GWB9ApTPTv4</a>, which I found to be really good. The Llama documentation is also great: <a href=https://github.com/ollama/ollama/blob/main/README.md>https://github.com/ollama/ollama/blob/main/README.md</a>.</p><p>So far, I have tried running Llama 3.2 7B and CodeGemma 7B. After getting used to Chat GPT, this seems like a significant slowdown. However, given that I have specific use cases in mind, it might still be suitable for me.</p><h2 id=my-use-cases>My use cases<a hidden class=anchor aria-hidden=true href=#my-use-cases>#</a></h2><h3 id=1-bookmarks>1. Bookmarks<a hidden class=anchor aria-hidden=true href=#1-bookmarks>#</a></h3><p>A while ago, I created software that served as a bookmark manager. It had two parts - a Telegram bot and a web interface. The way it worked was that you would send a web URL, and Python would scrape the website for the title and assign tags based on some rules. For example, if the domain name was YouTube, it assigned &ldquo;video&rdquo;, or if the title contained words like &ldquo;Python&rdquo;, it assigned &ldquo;programming&rdquo; and &ldquo;Python&rdquo;. It used to run on Heroku until they discontinued their free tier. The Telegram bot used to run on my Raspberry Pi at home. However, the database is still hosted on Cloud MongoDB, so that&rsquo;s the only part that remains active now. What I would like to do next is create a new version of the bookmarks manager, and I plan to use LLM to summarize the content of each page and auto-assign tags. I&rsquo;ll dedicate a separate series of posts under the tag &ldquo;bookmarx&rdquo; to this project.</p><h3 id=2-document-retriever>2. Document retriever<a hidden class=anchor aria-hidden=true href=#2-document-retriever>#</a></h3><p>I have a large number of different documents, and sometimes I simply forget where they are or how to find them. Moreover, I often only need a specific part of the information from that document. For example, I need to descale my coffee machine periodically, but I don&rsquo;t store that information in my head because I know it&rsquo;s available in the manual or can be found online. However, I&rsquo;ve noticed that Google has become less effective lately, and sometimes it takes around 10 minutes to find what I&rsquo;m looking for. Another example is my personal documents, such as visas, emails, or flight information, which are buried in my Gmail inbox. Unfortunately, my Gmail account contains a lot of unnecessary information, so the search results often include both relevant and irrelevant data.</p><h3 id=3-expenses-and-budget-planner>3. Expenses and budget planner<a hidden class=anchor aria-hidden=true href=#3-expenses-and-budget-planner>#</a></h3><p>I&rsquo;ve created my own budget planner app, which was quite good at allowing me to create different scenarios. However, I found that without actual expense data, it&rsquo;s difficult to make accurate judgments about my financial situation. The reason I haven&rsquo;t used any existing solutions on the market is that I lack trust in them. Additionally, I use multiple banks for different purposes, so having all my information in one place would be convenient. To address this issue, I tried creating a custom CV model that could read the PDFs exported by my bank and convert them into a list of transactions. Unfortunately, even within a single bank, the output of the PDF is not consistent, requiring adjustments to be made. I was thinking of trying a multi-modal LLM to combat this issue and at least get all my transactions in a table format for starters.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://timurvafin.blog/tags/macbook-pro/>Macbook Pro</a></li><li><a href=https://timurvafin.blog/tags/ollama/>Ollama</a></li><li><a href=https://timurvafin.blog/tags/llm/>Llm</a></li></ul><nav class=paginav><a class=prev href=https://timurvafin.blog/posts/taskmanagers/><span class=title>« Prev</span><br><span>Task managers</span>
</a><a class=next href=https://timurvafin.blog/posts/upgadingmac/><span class=title>Next »</span><br><span>Upgrading OS on my MacbookPro Mid-2012</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://timurvafin.blog/>Timur Coding Stuff</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>